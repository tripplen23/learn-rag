{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This Notebook is to demonstrate commonly used Loaders and Splitters\n",
    "#### In LangChain, a Document is a simple structure with 2 fields:\n",
    "- **page_content (string)**: This field contains the raw text of the document.\n",
    "- **metadata (dictionary)**: This field stores additional metadata about the text, such as the source URL, author, or any other relevant information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import TextLoader\n",
    "\n",
    "# Load a document from a text file using TextLoader.\n",
    "loader = TextLoader(\"./loaders-samples/sample.txt\")\n",
    "document = loader.load()\n",
    "print(document)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "document[0].page_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "document[0].metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Types of Document Loaders in LangChain\n",
    "#### LangChain offers three main types of Document Loaders:\n",
    "- **Transform Loaders**: These loaders handle different input formats and transform them into the Document format. For instance, consider a CSV file named \"data.csv\" with columns for \"name\" and \"age\". Using the CSVLoader, you can load the CSV data into Documents.\n",
    "- **Public Dataset or Service Loaders**: LangChain provides loaders for popular public sources, allowing quick retrieval and creation of Document. For example, the WikipediaLoader can load content from Wikipedia.\n",
    "- **Proprietary Dataset or Service Loaders**: These loaders are designed to handle proprietary sources that may require additional authentication or setup. For instance, a loader could be created specifically for loading data from an internal database or an API with proprietary access."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Transform Loader example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CSVLoader\n",
    "\n",
    "from langchain.document_loaders import CSVLoader\n",
    "\n",
    "# Load data from a CSV file using CSVLoader\n",
    "loader = CSVLoader(\"../csv/HR-Employee-Attrition.csv\")\n",
    "documents = loader.load()\n",
    "\n",
    "# Access the content and metadata of each document\n",
    "for document in documents:\n",
    "  content = document.page_content\n",
    "  metadata = document.metadata\n",
    "\n",
    "  print(content)\n",
    "  print(\"-----------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### PDFLoader\n",
    "Loads each page of the PDF as one document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "\n",
    "loader = PyPDFLoader(\"../pdf/CV (1).pdf\")\n",
    "pages = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnt = 0\n",
    "for page in pages:\n",
    "  cnt = cnt + 1\n",
    "  print(\"---- Document #\", cnt)\n",
    "  print(page.page_content.strip())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### WebBaseLoader\n",
    "This covers how to use WebBaseLoader to load all text from HTML webpages into a document format that we can use downstream."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "\n",
    "loader = WebBaseLoader(\"https://www.ibm.com/\")\n",
    "data = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[0].page_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine strip() with string formatting for basic formatting\n",
    "formatted_text = data[0].page_content.strip().replace(\"\\n\\n\", \"\\n\")  # Replace double newlines with single newlines\n",
    "\n",
    "print(formatted_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use regular expressions for more comprehensive cleaning:\n",
    "import re\n",
    "\n",
    "# Remove unnecessary whitespace and multiple newlines\n",
    "cleaned_text = re.sub(r\"\\s+\", \" \", formatted_text)  # Replace multiple spaces with single space\n",
    "cleaned_text = re.sub(r\"\\n+\", \"\\n\\n\", cleaned_text)  # Limit newlines to two per paragraph\n",
    "\n",
    "print(cleaned_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### JSON Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import JSONLoader\n",
    "import json\n",
    "from pathlib import Path\n",
    "from pprint import pprint\n",
    "\n",
    "file_path = \"./loaders-samples/sample.json\"\n",
    "data = json.loads(Path(file_path).read_text())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pprint(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = loader = JSONLoader(\n",
    "  file_path=\"./loaders-samples/sample.json\",\n",
    "  jq_schema=\".employees[].email\",\n",
    "  text_content=False\n",
    ")\n",
    "\n",
    "data = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Public Dataset or Service Loaders\n",
    "\n",
    "#### Wikipedia Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import WikipediaLoader\n",
    "\n",
    "# Load content from Wikipedia using WikipediaLoader\n",
    "loader = WikipediaLoader(\"Machine_learning\")\n",
    "document = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "document[0].page_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "document[0].metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### IMDB Movie Script Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import IMSDbLoader\n",
    "loader = IMSDbLoader(\"https://imsdb.com/scripts/BlacKkKlansman.html\")\n",
    "\n",
    "data = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove unnecessary newlines and carriage returns\n",
    "formatted_text = data[0].page_content[:5000].strip()\n",
    "\n",
    "# Print the formatted text\n",
    "print(formatted_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### YouTubeLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install --upgrade --quiet  youtube-transcript-api"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import YoutubeLoader\n",
    "\n",
    "loader = YoutubeLoader.from_youtube_url(\n",
    "  \"https://www.youtube.com/watch?v=QsYGlZkevEg\", add_video_info=False\n",
    ")\n",
    "\n",
    "data = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove unnecessary newlines and carriage returns\n",
    "formatted_text = data[0].page_content[:5000].strip()\n",
    "\n",
    "# Print the formatted text\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Add Video preferences, Add language preferences\n",
    "- Language param: It's a list of language codes in a descending priority, en by default.\n",
    "- Translation param: It's a translate preference, you can translate available transcript to your preferred language."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = YoutubeLoader.from_youtube_url(\n",
    "    \"https://www.youtube.com/watch?v=IkfPtvA6RmA\",\n",
    "    add_video_info=True,\n",
    "    language=[\"en\", \"id\"],\n",
    "    translation=\"en\",\n",
    ")\n",
    "ytdata = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ytdata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove unnecessary newlines and carriage returns\n",
    "formatted_text = ytdata[0].page_content[:5000].strip()\n",
    "\n",
    "# Print the formatted text\n",
    "print(formatted_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text Splitters\n",
    "\n",
    "Once you've loaded documents, you'll often want to transform them to better suit your application. The simplest example is you may want to split a long document into smaller chunks that can fit into your model's context window. LangChain has a number of built-in document transformers that make it easy to split, combine, filter, and otherwise manipulate documents.\n",
    "\n",
    "When you want to deal with long pieces of text, it is necessary to split up that text into chunks. As simple as this sounds, there is a lot of potential complexity here. Ideally, you want to keep the semantically related pieces of text together. What \"sematically related\" means could depend on the type of text. This notebook showcases several ways to do that.\n",
    "\n",
    "At a high level, text splitters work as following:\n",
    "- Split the text up into small, semantically meaningful chunks (often sentences).\n",
    "- Start combine these small chunks into larger chunk ultil you reach a certain size (as measured by some function).\n",
    "- Once you reach that size, make that chunk its own piece of text and then start creating a new chunk of text with some overlap (to keep context between chunks).\n",
    "\n",
    "That means there are 2 different axes along which you can customize your text splitter:\n",
    "- How the text is split\n",
    "- How the chunk size is measured"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "\n",
    "text_splitter = CharacterTextSplitter(\n",
    "  separator=\"\\n\\n\",\n",
    "  chunk_size=200,\n",
    "  chunk_overlap=20,\n",
    "  length_function=len,\n",
    "  is_separator_regex=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = WebBaseLoader(\"https://www.ibm.com/\")\n",
    "data = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunks = text_splitter.split_text(data[0].page_content)\n",
    "len(chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for chunk in chunks:\n",
    "  print(chunk)\n",
    "  print(\"---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = text_splitter.create_documents([data[0].page_content])\n",
    "len(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for doc in documents:\n",
    "  print(doc)\n",
    "  print(\"---\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### RecursiveCharacterTextSplitter\n",
    "This text splitter is the recommendation one for generic text. It is parameterized by a list of characters. It tries to split on them in order until the chunks are small enough. The default list is [\"\\n\\n\", \"\\n\", \" \", \"\"]. This has the effect of trying to keep all paragraphs (and then sentences, and then words) together as long as possible, as those would generically seem to be the strongest semantically related pieces of text.\n",
    "\n",
    "- How the text is split: By list of characters.\n",
    "- How the chunk size is measured: By number of characters\n",
    "- The RecursiveCharacterTextSplitter class does use chunk_size and overlap parameters to split the text into chunks of the specified size and overlap. This is because its split_text recursively splits the text based on different seperators until the length of splits is less than the chunk_size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "rectext_splitter = RecursiveCharacterTextSplitter(\n",
    "  # Set a really small chunk size, just to show.\n",
    "  chunk_size=100,\n",
    "  chunk_overlap=20,\n",
    "  length_function=len,\n",
    "  is_separator_regex=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = rectext_splitter.create_documents([data[0].page_content])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for text in texts:\n",
    "  print(text)\n",
    "  print(\"---\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
